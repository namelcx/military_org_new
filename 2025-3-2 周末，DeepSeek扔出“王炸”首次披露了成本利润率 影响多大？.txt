周六，国内AI大模型公司DeepSeek官方账号在知乎首次发布《DeepSeek-V3/R1推理系统概览》技术文章，不仅公开了其推理系统的核心优化方案，更是首次披露了成本利润率等关键数据，引发行业震动。

　　数据显示，若按理论定价计算，其单日成本利润率高达545%，这一数字刷新了全球AI大模型领域的盈利天花板。

　　业内分析指出，DeepSeek的开源策略与成本控制能力正在打破AI领域的资源垄断。DeepSeek此次“透明化”披露，不仅展示了其技术实力与商业潜力，更向行业传递明确信号：AI大模型的盈利闭环已从理想照进现实。

　　DeepSeek最新发布

　　3月1日，DeepSeek于知乎开设官方账号，发布《DeepSeek-V3/R1推理系统概览》技术文章，首次公布模型推理系统优化细节，并披露成本利润率关键信息。

　　文章写道：“DeepSeek-V3/ R1推理系统的优化目标是：更大的吞吐，更低的延迟。”

　　为实现这两个目标，DeepSeek的方案是使用大规模跨节点专家并行（EP），但该方案也增加了系统复杂性。文章的主要内容就是关于如何使用EP增长批量大小（batch size）、隐藏传输耗时以及进行负载均衡。

　　值得注意的是，文章还率先披露了DeepSeek的理论成本和利润率等关键信息。

　　根据DeepSeek官方披露，DeepSeek V3和R1的所有服务均使用H800 GPU，使用和训练一致的精度，即矩阵计算和dispatch 传输采用和训练一致的FP8格式，core-attention计算和combine传输采用和训练一致的BF16，最大程度保证了服务效果。

　　另外，由于白天的服务负荷高，晚上的服务负荷低，因此DeepSeek实现了一套机制，在白天负荷高的时候，用所有节点部署推理服务。晚上负荷低的时候，减少推理节点，以用来做研究和训练。

　　在最近24小时（2025年2月27日12:00至28日12:00）的统计周期内：GPU租赁成本按2美元/小时计算，日均成本为87072美元；若所有输入/输出token按R1定价（输入1元/百万token、输出16元/百万token）计算，单日收入可达562027美元，成本利润率高达545%。


　　不过，DeepSeek官方坦言，实际上没有这么多收入，因为V3的定价更低，同时收费服务只占了一部分，另外夜间还会有折扣。

　　DeepSeek的高利润率源于其创新的推理系统设计，核心包括大规模跨节点专家并行（EP）、计算通信重叠与负载均衡优化三大技术支柱：专家并行（EP）提升吞吐与响应速度，针对模型稀疏性（每层仅激活8/256个专家），采用EP策略扩展总体批处理规模（batch size），确保每个专家获得足够的计算负载，显著提升GPU利用率；部署单元动态调整（如Prefill阶段4节点、Decode阶段18节点），平衡资源分配与任务需求。

　　计算与通信重叠隐藏延迟，Prefill阶段通过“双batch交错”实现计算与通信并行，Decode阶段拆分attention为多级流水线，最大限度掩盖通信开销。

　　全局负载均衡避免资源浪费，针对不同并行模式（数据并行DP、专家并行EP）设计动态负载均衡器，确保各GPU的计算量、通信量及KVCache占用均衡，避免节点空转。

　　简单来说，EP就像是“多人协作”，把模型中的“专家”分散到多张GPU上进行计算，大幅提升Batch Size，榨干GPU算力，同时专家分散，降低内存压力，更快响应。

　　DeepSeek在工程层面进一步压缩成本。昼夜资源调配：白天高峰时段全力支持推理服务，夜间闲置节点转用于研发训练，最大化硬件利用率；缓存命中率达56.3%：通过KVCache硬盘缓存减少重复计算，在输入token中，有3420亿个（56.3%）直接命中缓存，大幅降低算力消耗。

　　影响多大？

　　有分析称，DeepSeek此次披露的数据，不仅验证了其技术路线的商业可行性，更为行业树立了高效盈利的标杆：其模型训练成本仅为同类产品的1%—5%，此前发布的DeepSeek-V3模型训练成本仅557.6万美元，远低于OpenAI等巨头；推理定价优势方面，DeepSeek-R1的API定价仅为OpenAI o3-mini的1/7至1/2，低成本策略加速市场渗透。

　　业内分析指出，DeepSeek的开源策略与成本控制能力正在打破AI领域的资源垄断。DeepSeek此次“透明化”披露，不仅展示了其技术实力与商业潜力，更向行业传递明确信号：AI大模型的盈利闭环已从理想照进现实，标志着AI技术从实验室迈向产业化的关键转折。

　　中信证券(27.330, -0.91, -3.22%)认为，Deepseek在模型训练成本降低方面的最佳实践，料将刺激科技巨头采用更为经济的方式加速前沿模型的探索和研究，同时将使得大量AI应用得以解锁和落地。算法训练带来的规模报酬递增效应以及单位算力成本降低对应的杰文斯悖论等，均意味着中短期维度科技巨头继续在AI算力领域进行持续、规模投入仍将是高确定性事件。

　　本周以来，DeepSeek开启“开源周”，给人工智能领域扔下数颗“重磅炸弹”。回顾DeepSeek这五天开源的内容，信息量很大，具体来看：

　　周一，DeepSeek宣布开源FlashMLA。FlashMLA是DeepSeek用于Hopper GPU的高效MLA解码内核，并针对可变长度序列进行了优化，现已投入生产；

　　周二，DeepSeek宣布开源DeepEP，即首个用于MoE模型训练和推理的开源EP通信库，提供高吞吐量和低延迟的all-to-all GPU内核；

　　周三，DeepSeek宣布开源DeepGEMM。其同时支持密集布局和两种MoE布局，完全即时编译，可为V3/R1模型的训练和推理提供强大支持等；

　　周四，DeepSeek宣布开源Optimized Parallelism Strategies。其主要针对大规模模型训练中的效率问题；

　　周五，DeepSeek宣布开源Fire-Flyer文件系统（3FS），以及基于3FS的数据处理框架Smallpond。

　　因此，有网友评论称：“《DeepSeek-V3/R1推理系统概览》技术文章是‘开源周彩蛋’，直接亮出了底牌！”